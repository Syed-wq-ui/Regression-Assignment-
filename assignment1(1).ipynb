{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raUea7LocaLi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS1 : Simple Linear Regression is a statistical method used to model the relationship between two variables:\n",
        "\n",
        "Independent variable (X) — the predictor or input.\n",
        "\n",
        "Dependent variable (Y) — the outcome or response.\n",
        "\n",
        "ANS2 :The key assumptions of linear regression include linearity, independence, homoscedasticity, and normality of residuals. These assumptions ensure the validity and reliability of the regression model and its results.\n",
        "\n",
        "ANS3 :In the equation y = mx + c, the coefficient 'm' represents the slope or gradient of the line\n",
        "\n",
        "ANS4 :In the equation y = mx + c, the variable 'c' represents the y-intercept of the line.\n",
        "\n",
        "ANS5 :In simple linear regression, the slope m is calculated using the formula, m = r * (sy / sx), where r is the correlation coefficient, sy is the standard deviation of the dependent variable, and sx is the standard deviation of the independent variable.\n",
        "\n",
        "ANS 6:The least squares method in simple linear regression aims to find the \"line of best fit\" by minimizing the sum of squared errors between the observed data points and the predicted values on the line\n",
        "\n",
        "ANS 7:The coefficient of determination (R²) measures how well a statistical model predicts an outcome. The outcome is represented by the model's dependent variable.\n",
        "\n",
        "ANS 8:Multiple linear regression is a statistical method used to model the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "ANS 9:The primary difference between simple and multiple linear regression lies in the number of independent variables used to predict a dependent variable.\n",
        "\n",
        "ANS 10:The key assumption for multiple linear regression is the linearity assumption. This means that the relationship between the dependent variable and each of the independent variables should be linear.\n",
        "\n",
        "ANS 11:Heteroscedasticity in a multiple linear regression model refers to the non-constant variance of the error terms (residuals) across different values of the independent variables.\n",
        "\n",
        "ANS12 :To improve a multiple linear regression model with high multicollinearity, consider these steps: first, identify the multicollinear variables using Variance Inflation Factor (VIF) or correlation matrices. Then, address the multicollinearity by either removing one of the correlated variables, creating new features by combining them.\n",
        "\n",
        "ANS13 :Several techniques can transform categorical variables for use in linear models. Two common methods are one-hot encoding and label encoding. One-hot encoding creates binary columns for each category, while label encoding assigns numerical values to categories.\n",
        "\n",
        "ANS14 :In multiple linear regression, interaction terms reveal how the effect of one independent variable on the dependent variable changes depending on the values of another independent variable. Essentially, they capture the non-additive relationship between predictors, meaning the combined influence of two variables on the outcome is not simply the sum of their individual effects.\n",
        "\n",
        "ANS15 :In simple regression, the intercept is often interpreted as the baseline value of the dependent variable when the single independent variable is zero. In multiple regression, the intercept represents the predicted value of the dependent variable when all independent variables are simultaneously zero, which might be a less practically relevant scenario than in simple regression.\n",
        "\n",
        "ANS16 :In regression analysis, the slope represents the change in the dependent variable (y) for every unit change in the independent variable (x). A steeper slope indicates a stronger relationship between the variables, meaning that a small change in x leads to a larger change in y. The slope directly impacts predictions: the higher the slope, the greater the predicted change in y for a given change in x.\n",
        "\n",
        "ANS17 :The intercept provides a baseline or starting point for understanding the relationship between variables. It represents the predicted value of the dependent variable when the independent variable is zero. This context helps establish a reference point for understanding how the dependent variable changes as the independent variable changes.\n",
        "\n",
        "ANS18 :R-squared, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit of a regression model. It represents the proportion of variance in the dependent variable that is predictable from the independent variable(s). While R-squared is widely used, it has limitations that can lead to misinterpretation of model performance.\n",
        "\n",
        "ANS19 :To find the standard error of the coefficients, you need to use the formula provided by a statistical software package like R or Python.\n",
        "\n",
        "ANS20:Heteroscedasticity, or unequal error variance, can be identified visually by plotting residuals against predicted values or fitted values. A cone or fan shape in the residual plot suggests heteroscedasticity. Statistical tests like the Breusch-Pagan test or White test can also confirm its presence.\n",
        "\n",
        "ANS21 :r-squared measures the proportion of variance in the dependent variable explained by the independent variables, it always increases when more predictors are added. Adjusted r-squared adjusts for the number of predictors and decreases if the additional variables do not contribute to the model's significance.\n",
        "\n",
        "ANS22 :Heteroscedasticity, or unequal error variance, can be identified visually by plotting residuals against predicted values or fitted values. A cone or fan shape in the residual plot suggests heteroscedasticity. Statistical tests like the Breusch-Pagan test or White test can also confirm its presence.\n",
        "\n",
        "ANS23 :Scaling variables in multiple linear regression is beneficial for several reasons, including faster convergence of optimization algorithms like gradient descent, improved interpretability of coefficients, and preventing dominant features from overshadowing others.\n",
        "\n",
        "ANS24 :Polynomial regression differs from linear regression by its ability to model non-linear relationships between variables. While linear regression seeks a straight-line fit, polynomial regression uses polynomial equations to fit curved relationships, making it suitable when the data exhibits non-linear patterns.\n",
        "\n",
        "ANS25 :Polynomial regression is used when the relationship between variables is non-linear and can be modeled by a polynomial function, such as a quadratic, cubic, or higher-degree curve. It's a form of regression analysis that allows for more complex relationships than linear regression.\n",
        "\n",
        "ANS26 :The general equation for polynomial regression is y = β₀ + β₁x + β₂x² + ... + βnxⁿ + ε, where y is the dependent variable, x is the independent variable, β₀, β₁, β₂, ..., βn are coefficients, n is the degree of the polynomial, and ε is the error term.\n",
        "\n",
        "ANS27: Yes, polynomial regression can be applied to multiple variables, and this is often referred to as Multivariate Polynomial Regression (MPR). MPR allows you to model not just the relationships between individual variables and the outcome, but also the interactions between the variables themselves.\n",
        "\n",
        "ANS28 :Polynomial regression, while a powerful tool, has limitations like overfitting, computational complexity, and potential issues with extrapolation. Overfitting occurs when high-degree polynomials fit the training data too closely, including noise, leading to poor generalization on unseen data.\n",
        "\n",
        "ANS29 :To evaluate model fit when selecting the degree of a polynomial, methods like R-squared, AIC, BIC, and cross-validation are used to assess how well the model fits the data and to avoid overfitting or underfitting.\n",
        "\n",
        "ANS 30 : Visualization is crucial in polynomial regression to understand data trends, assess the model's fit, and identify potential issues like overfitting or underfitting. By visualizing the data and the model's predictions, you can gain insights into the underlying relationship between variables, which is particularly helpful when dealing with curved patterns that linear regression cannot capture.\n",
        "\n",
        "ANS31 :Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. It is used when the relationship between the variables is non-linear.\n"
      ],
      "metadata": {
        "id": "jnN3HegVchyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "93ecdzKStUg2"
      }
    }
  ]
}